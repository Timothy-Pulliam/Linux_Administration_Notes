Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2019-03-18T12:22:45-04:00

====== storage ======
Created Monday 18 March 2019

==== RAID ====
BBU = backup battery unit (RAID controller battery)

We highly recommend to use a hardware RAID controller (with BBU) for such setups. This increases performance, provides redundancy, and make disk replacements easier (hot-pluggable).

==== SMART Disk Monitoring ====
You can get the status of a disk by issuing the following command:

''# smartctl -a /dev/sdX''

If the output says:

''SMART support is: Disabled''

you can enable it with the command:

''# smartctl -s on /dev/sdX''

By default, smartmontools daemon smartd is active and enabled, and scans the disks under /dev/sdX and /dev/hdX every 30 minutes for errors and warnings, and sends an e-mail to root if it detects a problem.

If you use your hard disks with a hardware raid controller, there are most likely tools to monitor the disks in the raid array and the array itself. For more information about this, please refer to the vendor of your raid controller.

==== Creating a Volume Group From a New Disk ====
To create a Logical Volume to store the VMs on:

First create a partition.

# sgdisk -N 1 [[/dev/sdb]]
or
# gdisk [[/dev/sdb]]

Create a Physical Volume (PV) without confirmation and 250K metadatasize.

# pvcreate --metadatasize 250k -y -ff [[/dev/sdb1]]

Create a volume group named “vmdata” on [[/dev/sdb1.]] This is where our VMs will be stored.

# vgcreate vmdata [[/dev/sdb1]]



== Create a LVM-thin pool ==
A thin pool has to be created on top of a volume group. How to create a volume group see Section LVM.

# lvcreate -L 80G -T -n vmstore vmdata

== Resizing the thin pool ==
Resize the LV and the metadata pool can be achieved with the following command.

# lvresize --size +<size[\M,G,T]> --poolmetadatasize +<size[\M,G]> <VG>/<LVThin_pool>


==== Default Post Install Configuration ====
The pve installer creates three logical volumes. The data volume is an LVM thin-pool volume (note the "t" attribute). This volume uses LVM-thin, and is used to store VM images. LVM-thin is preferable for this task, because it offers efficient support for snapshots and clones.

root@pve1-gkh8ww1:~# lvs                                                                       
'''
LV   VG  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert               
data pve twi-a-tz-- 75.87g             0.00   0.04                                           
root pve -wi-ao---- 33.75g                                                                   
swap pve -wi-ao----  8.00g
'''
                                                    

==== /etc/pve/storage.conf ====
All Proxmox VE related storage configuration is stored within a single text file at /etc/pve/storage.cfg. As this file is within /etc/pve/, it gets automatically distributed to all cluster nodes. So all nodes share the same storage configuration.

Sharing storage configuration make perfect sense for shared storage, because the same “shared” storage is accessible from all nodes. But is also useful for local storage types. In this case such local storage is available on all nodes, but it is physically different and can have totally different content.

cat /etc/pve/storage.conf
'''
dir: local
        path /var/lib/vz
        content iso,vztmpl,backup

lvmthin: local-lvm
        thinpool data
        vgname pve
        content rootdir,images
'''

{{.\pasted_image.png}}

Notice the data logical volume in the pve volume group matches the entry in storage.cfg and the drop down for a new vm.

'''
root@pve1-gkh8ww1:/var/lib/vz/template/iso# lvs
  LV   VG  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  
'''
''data pve twi-a-tz-- 75.87g             0.00   0.04''                            
''  root pve -wi-ao---- 33.75g                                                    ''
''  swap pve -wi-ao----  8.00g''                                  
